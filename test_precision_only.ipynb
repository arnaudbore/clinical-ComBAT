{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS and UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from scripts import combat_info\n",
    "\n",
    "\n",
    "CAMCAN = \"./DONNES/CamCAN.md.raw.csv.gz\"\n",
    "\n",
    "\n",
    "MAINFOLDER = \"ROBUST_EVALUATION\"\n",
    "SYNTHETIC_SITES = f\"{MAINFOLDER}/SYNTHETIC_SITES\"\n",
    "\n",
    "\n",
    "\n",
    "RAWFOLDER = \"RAW\"\n",
    "\n",
    "ANALYSISFOLDER = \"ANALYSIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(mov_data_file):\n",
    "    [df,bundles] = combat_info.info(mov_data_file)\n",
    "    nb_hc = int(re.findall('HC\\(n=(\\d+)',df[\"DetailInfos\"][\"Disease\"])[0])\n",
    "    nb_total = df[\"DetailInfos\"][\"Number of Subject\"]\n",
    "    nb_sick = nb_total - nb_hc\n",
    "    return [nb_total,nb_hc,nb_sick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundles(mov_data_file):\n",
    "    return combat_info.get_bundles(mov_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_text(x):\n",
    "    return \"NoRobust\" if x == 'No' else x\n",
    "\n",
    "def rwp_text(x):\n",
    "    return \"RWP\" if x else \"NoRWP\"\n",
    "def get_site(mov_data_file):\n",
    "    mov_data = pd.read_csv(mov_data_file)\n",
    "    return mov_data.site.unique()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nb_patients_and_diseased(df):\n",
    "  df['num_patients'] = df['site'].str.extract(r'(\\d+)_patients')[0].astype(int)\n",
    "  df['disease_ratio'] = df['site'].str.extract(r'(\\d+)_percent')[0].astype(int)\n",
    "  df['num_diseased'] = (df['num_patients'] * df['disease_ratio']/100).astype(int)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df1,df2, title, bundle='mni_MCP'):\n",
    "    df1_bundle = df1[df1['bundle'] == bundle]\n",
    "    df2_bundle = df2[df2['bundle'] == bundle]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(df1_bundle['age'], df1_bundle['mean'], label='Train', alpha=0.5, color='green')\n",
    "    plt.scatter(df2_bundle['age'], df2_bundle['mean'], label='Test', alpha=0.5, color='red')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SITE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(file_path, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training and testing sets, ensuring the same proportion of HC and non-HC patients\n",
    "    and that data from the same sid are in the same dataset.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to split.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Training set.\n",
    "    pd.DataFrame: Testing set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Group by 'sid' and get unique sids\n",
    "    unique_sids = df.groupby('sid').first().reset_index()\n",
    "    \n",
    "    # Split the unique sids into train and test sets\n",
    "    train_sids, test_sids = train_test_split(unique_sids, test_size=test_size, random_state=random_state, stratify=unique_sids['disease'])\n",
    "    \n",
    "    # Create train and test DataFrames by filtering the original DataFrame\n",
    "    train_df = df[df['sid'].isin(train_sids['sid'])]\n",
    "    test_df = df[df['sid'].isin(test_sids['sid'])]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_patients(df, num_patients, disease_ratio,index):\n",
    "    # Lire le fichier CSV dans un DataFrame\n",
    "    \n",
    "    # Calculer le nombre de patients malades et sains\n",
    "    num_diseased = int(num_patients * disease_ratio)\n",
    "    num_healthy = num_patients - num_diseased\n",
    "    \n",
    "    # Filtrer les patients en santé (HC) et malades\n",
    "    healthy_patients = df[df['disease'] == 'HC']\n",
    "    diseased_patients = df[df['disease'] != 'HC']\n",
    "    \n",
    "    # S'assurer qu'il y a assez de patients pour chaque catégorie\n",
    "    if len(healthy_patients['sid'].unique()) < num_healthy or len(diseased_patients['sid'].unique()) < num_diseased:\n",
    "        raise ValueError(\"Nombre insuffisant de patients en santé ou malades pour l'échantillon demandé.\")\n",
    "    \n",
    "    # Sélectionner un échantillon aléatoire de patients sains et malades\n",
    "    sampled_healthy = healthy_patients.groupby('sid').sample(frac=1).head(num_healthy * df['bundle'].nunique())\n",
    "    sampled_diseased = diseased_patients.groupby('sid').sample(frac=1).head(num_diseased * df['bundle'].nunique())\n",
    "    \n",
    "    # Combiner les échantillons pour obtenir le DataFrame final\n",
    "    sampled_df = pd.concat([sampled_healthy, sampled_diseased])\n",
    "    # Modifier les valeurs de 'site' pour toutes les lignes\n",
    "    sampled_df['site'] = f\"{num_patients}_patients_{int(disease_ratio*100)}_percent_{index}\"\n",
    "    \n",
    "    # Retourner le DataFrame final\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biaised_data(df1, df2, \n",
    "                additive_uniform_low=-3, additive_uniform_high=3, \n",
    "                multiplicative_uniform_low=0.5, multiplicative_uniform_high=2, \n",
    "                additive_std_low=0.01, additive_std_high=0.1, \n",
    "                multiplicative_std_low=0.01, multiplicative_std_high=0.1):\n",
    "    \"\"\"\n",
    "    Génère des biais additifs et multiplicatifs pour chaque bundle en fonction de df1, puis applique ces biais à df1 et df2\n",
    "    de manière indépendante en tenant compte des covariables (âge, sexe, latéralité) et en centrant les résidus.\n",
    "\n",
    "    Parameters:\n",
    "    - df1, df2 (pd.DataFrame): Les DataFrames sur lesquels appliquer les biais.\n",
    "    - additive_uniform_low, additive_uniform_high : paramètres pour le biais additif.\n",
    "    - multiplicative_uniform_low, multiplicative_uniform_high : paramètres pour le biais multiplicatif.\n",
    "    - additive_std_low, additive_std_high : paramètres pour l'écart-type du biais additif.\n",
    "    - multiplicative_std_low, multiplicative_std_high : paramètres pour l'écart-type du biais multiplicatif.\n",
    "\n",
    "    Returns:\n",
    "    - tuple : Deux DataFrames avec les biais appliqués indépendamment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionnaires pour stocker les biais par bundle\n",
    "    additive_bias_per_bundle = {}\n",
    "    multiplicative_bias_per_bundle = {}\n",
    "\n",
    "    # # Tirer les moyennes de biais de distributions uniformes pour le bundle\n",
    "    additive_mean = np.random.uniform(low=additive_uniform_low, high=additive_uniform_high)\n",
    "    multiplicative_mean = np.random.uniform(low=multiplicative_uniform_low, high=multiplicative_uniform_high)\n",
    "    \n",
    "    # # Tirer les écarts-types de biais de distributions uniformes pour le bundle\n",
    "    additive_std = np.random.uniform(low=additive_std_low, high=additive_std_high)\n",
    "    multiplicative_std = np.random.uniform(low=multiplicative_std_low, high=multiplicative_std_high)\n",
    "\n",
    "    # Calcul des biais pour chaque bundle unique dans df1\n",
    "    for bundle in df1['bundle'].unique(): \n",
    "        # Générer un biais additif et multiplicatif spécifique au bundle\n",
    "        additive_bias_per_bundle[bundle] = np.random.normal(loc=additive_mean, scale=additive_std)\n",
    "        multiplicative_bias_per_bundle[bundle] = np.random.normal(loc=multiplicative_mean, scale=multiplicative_std)\n",
    "   \n",
    "    # Appliquer les biais indépendamment à df1 et df2 en utilisant les mêmes biais générés\n",
    "    combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    biased_df = apply_bias(combined, additive_bias_per_bundle, multiplicative_bias_per_bundle)\n",
    "    biased_df1 = biased_df[biased_df['sid'].isin(df1['sid'])]\n",
    "    biased_df2 = biased_df[biased_df['sid'].isin(df2['sid'])]\n",
    "    bias_parameters = {\n",
    "        'additive_mean': additive_mean,\n",
    "        'multiplicative_mean': multiplicative_mean,\n",
    "        'additive_std': additive_std,\n",
    "        'multiplicative_std': multiplicative_std\n",
    "    }\n",
    "    \n",
    "    return biased_df1, biased_df2, additive_bias_per_bundle, multiplicative_bias_per_bundle, bias_parameters\n",
    "\n",
    "def apply_bias(dataframe, additive_bias_per_bundle, multiplicative_bias_per_bundle):\n",
    "    biased_df = dataframe.copy()\n",
    "    \n",
    "    # Application de la régression et des biais pour chaque bundle unique\n",
    "    for bundle in biased_df['bundle'].unique():\n",
    "        # Filtrer le DataFrame pour le bundle actuel\n",
    "        bundle_df = biased_df[biased_df['bundle'] == bundle]\n",
    "\n",
    "        # Préparer les covariables pour la régression\n",
    "        X = bundle_df[['age', 'sex', 'handedness']]\n",
    "        y = bundle_df['mean']\n",
    "        \n",
    "        # Ajuster le modèle de régression linéaire pour le bundle\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculer les prédictions et les résidus pour le bundle\n",
    "        predicted_mean = model.predict(X)\n",
    "        residuals = y - predicted_mean\n",
    "\n",
    "        # Récupérer les biais pour le bundle actuel\n",
    "        additive_bias = additive_bias_per_bundle[bundle]\n",
    "        multiplicative_bias = multiplicative_bias_per_bundle[bundle]\n",
    "        \n",
    "        # Appliquer les biais aux résidus centrés et réintégrer les effets des covariables\n",
    "        biased_means_bundle = residuals * multiplicative_bias + additive_bias * np.std(residuals) + predicted_mean\n",
    "        biased_df.loc[biased_df['bundle'] == bundle, 'mean'] = biased_means_bundle\n",
    "    \n",
    "    # Assigner les valeurs biaisées calculées au DataFrame\n",
    "    return biased_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATE SITES\n",
    "def generate_sites(sample_sizes, disease_ratios, num_tests, SYNTHETIC_SITES_VERSION):\n",
    "    directory = os.path.join(SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION)\n",
    "    train_df, test_df = split_train_test(COMPILATION, test_size=0.2, random_state=42)\n",
    "    # Initialize DataFrames to store the results\n",
    "    for sample_size in sample_sizes:\n",
    "        for disease_ratio in disease_ratios:  \n",
    "            sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            for i in range(num_tests):\n",
    "                \n",
    "                tempDir = os.path.join(sizeDir, f\"{i}\")\n",
    "                os.makedirs(tempDir, exist_ok=True)\n",
    "\n",
    "                train_df_biaised, test_df_biaised, gammas, deltas, parameters= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "                sampled_df_biaied =  sample_patients(train_df_biaised, sample_size, disease_ratio,i)\n",
    "\n",
    "                # Sauvegarder l'échantillon dans un fichier temporaire\n",
    "                temp_train_file = os.path.join(tempDir, f\"train_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "                sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "                \n",
    "                temp_test_file = os.path.join(tempDir, f\"test_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "                test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "                # Sauvegarde dans un fichier JSON\n",
    "                with open(os.path.join(tempDir,'parameters.json'), 'w') as file:\n",
    "                    json.dump({'parameters': parameters, 'gammas': gammas, 'deltas': deltas}, file, indent=4)\n",
    "\n",
    "                cmd = (\n",
    "                    \"scripts/combat_visualize_data.py\"\n",
    "                    + \" \"\n",
    "                    + COMPILATION\n",
    "                    + \" \"\n",
    "                    + temp_train_file\n",
    "                    + \" --out_dir \"\n",
    "                    + os.path.join(tempDir, \"VIZ\")\n",
    "                    + \" -f\"\n",
    "                    + \" --bundles all\"\n",
    "                )\n",
    "                #subprocess.call(cmd, shell=True)\n",
    "                cmd = (\n",
    "                    \"scripts/combat_visualize_data.py\"\n",
    "                    + \" \"\n",
    "                    + COMPILATION\n",
    "                    + \" \"\n",
    "                    + temp_test_file\n",
    "                    + \" --out_dir \"\n",
    "                    + os.path.join(tempDir, \"VIZ_TEST\")\n",
    "                    + \" -f\"\n",
    "                    + \" --bundles all\"\n",
    "                )\n",
    "                #subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARMONIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(mov_data_file, robust, rwp, directory, hc,):\n",
    "    ###########\n",
    "    ### fit ###\n",
    "    ###########\n",
    "    output_model_filename = (\n",
    "            get_site(mov_data_file)\n",
    "            + \".\"\n",
    "            + metric\n",
    "            + \".\"\n",
    "            + method\n",
    "            + \".\"\n",
    "            + robust_text(robust)\n",
    "            + \".\"\n",
    "            + rwp_text(rwp)\n",
    "            + \".model.csv\"\n",
    "        )\n",
    "    cmd = (\n",
    "        \"scripts/combat_quick_fit.py\"\n",
    "        + \" \"\n",
    "        + CAMCAN\n",
    "        + \" \"\n",
    "        + mov_data_file\n",
    "        + \" --out_dir \"\n",
    "        + directory\n",
    "        + \" --output_model_filename \"\n",
    "        + output_model_filename\n",
    "        + \" --method \"\n",
    "        + method\n",
    "        + \" --robust \"\n",
    "        + robust\n",
    "        + \" -f \"\n",
    "    )\n",
    "    if rwp:\n",
    "        cmd += ' --rwp'\n",
    "    if hc: \n",
    "        cmd += ' --hc'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    return output_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize(f_train, directory, robust, rwp,hc):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f_train)\n",
    "    \n",
    "    # Fit the model\n",
    "    fit(f_train, robust, rwp, directory, hc)\n",
    " \n",
    "    metrics_filename = os.path.join(directory, f\"metrics_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "    outliers_filename = os.path.join(directory, f\"outliers_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "    properties_filename = os.path.join(directory, f\"properties_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "    \n",
    "    # Load metrics from CSV file\n",
    "    loaded_metrics = pd.read_csv(metrics_filename)\n",
    "    \n",
    "    # Load outliers from CSV file\n",
    "    loaded_outliers_df = pd.read_csv(outliers_filename, index_col=0)\n",
    "\n",
    "    # Load properties from CSV file\n",
    "    loaded_properties = pd.read_csv(properties_filename)\n",
    "    \n",
    "    return loaded_metrics, loaded_outliers_df, loaded_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_site(f_train, robust, directory):\n",
    "\n",
    "    harmonization_robust = harmonize(f_train, os.path.join(directory, \"robust\"), robust, False, False)\n",
    "\n",
    "    #TODO bundles et analyze outliers\n",
    "    return harmonization_robust[0], harmonization_robust[1], harmonization_robust[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse Method\n",
    "def analyse_method(sample_sizes, disease_ratios, num_tests, robust_method, SYNTHETIC_SITES_VERSION):\n",
    "    # Split the data into training and testing sets\n",
    "    directory = os.path.join(MAINFOLDER, robust_method)\n",
    "    directory_site = os.path.join(SYNTHETIC_SITES ,SYNTHETIC_SITES_VERSION)\n",
    "    # Initialize DataFrames to store the results\n",
    "    metrics_compilation = pd.DataFrame()\n",
    "    properties_compilation = pd.DataFrame()\n",
    "    outliers_compilation = pd.DataFrame()\n",
    "    for sample_size in sample_sizes:\n",
    "        for disease_ratio in disease_ratios:        \n",
    "            sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            sizeDir_site = os.path.join(directory_site, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            for i in range(num_tests):\n",
    "                tempDir = os.path.join(sizeDir, f\"{i}\")\n",
    "                tempDir_site = os.path.join(sizeDir_site, f\"{i}\")\n",
    "                os.makedirs(tempDir, exist_ok=True)\n",
    "\n",
    "                train_file_name = f\"train_{sample_size}_{int(disease_ratio*100)}_{i}.csv\"\n",
    "                test_file_name = f\"test_{sample_size}_{int(disease_ratio*100)}_{i}.csv\"\n",
    "                \n",
    "                # Sauvegarder l'échantillon dans un fichier temporaire\n",
    "                temp_file = os.path.join(tempDir_site,train_file_name )\n",
    "                train_df = pd.read_csv(temp_file)\n",
    "                train_df.to_csv(os.path.join(tempDir,train_file_name ), index=False)\n",
    "\n",
    "                test_file = os.path.join(tempDir_site, test_file_name)\n",
    "                test_df = pd.read_csv(test_file)\n",
    "                test_df.to_csv(os.path.join(tempDir,test_file_name ), index=False)\n",
    "\n",
    "                \n",
    "                # Analyser le site pour le nouvel échantillon\n",
    "                metrics, outliers, properties = analyse_site(temp_file, robust_method, tempDir)\n",
    "                metrics_compilation = pd.concat([metrics_compilation, metrics])\n",
    "                outliers_compilation = pd.concat([outliers_compilation, outliers])\n",
    "                properties_compilation = pd.concat([properties_compilation, properties])\n",
    "\n",
    "    # Save the metrics and distances compilation DataFrames to CSV files\n",
    "    metrics_compilation.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n",
    "    outliers_compilation.to_csv(os.path.join(directory, \"outliers_compilation.csv\"), index=False)\n",
    "    properties_compilation.to_csv(os.path.join(directory, \"properties_compilation.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_group = 'ADNI'\n",
    "robust_method = 'IQR'\n",
    "metric = \"md\"\n",
    "method= \"classic\"\n",
    "\n",
    "SYNTHETIC_SITES_VERSION = \"v1\"\n",
    "\n",
    "COMPILATION = f\"./DONNES/adni_compilation.{metric}.csv.gz\"\n",
    "\n",
    "\n",
    "sample_sizes = [30, 50, 100, 150, 200, 300]  # Différentes tailles d'échantillon\n",
    "disease_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7]  # Différents pourcentages de malades\n",
    "#sample_sizes = [150, 300]  # Différentes tailles d'échantillon\n",
    "#disease_ratios = [0.3, 0.5]  # Différents pourcentages de malades\n",
    "num_tests = 30  # Nombre de tests à effectuer pour chaque combinaison\n",
    "\n",
    "#generate_sites(sample_sizes, disease_ratios, num_tests, SYNTHETIC_SITES_VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_method(sample_sizes, disease_ratios, num_tests, robust_method, SYNTHETIC_SITES_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne par site\n",
    "# Nothing really interesting so far\n",
    "directory = os.path.join(MAINFOLDER, robust_method)\n",
    "metrics_compilation = pd.read_csv(os.path.join(directory, \"metrics_compilation.csv\"))\n",
    "properties_compilation = pd.read_csv(os.path.join(directory, \"properties_compilation.csv\"))\n",
    "directory = os.path.join(directory, ANALYSISFOLDER)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "properties_compilation['site'] = properties_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Display the means by site\n",
    "metrics_means_by_site = metrics_compilation.groupby(['site', 'metric']).mean().reset_index()\n",
    "properties_means_by_site = properties_compilation.groupby(['site', 'property']).mean().reset_index()\n",
    "\n",
    "metrics_means_by_site.to_csv(os.path.join(directory, \"metrics_compilation_mean.csv\"), index=False)\n",
    "properties_means_by_site.to_csv(os.path.join(directory, \"properties_compilation_mean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE BEST BUNDLES for F1, precision etc\n",
    "def calculate_precision_by_bundle(df):\n",
    "    \"\"\"\n",
    "    Calcule le score de précision par bundle.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données avec les colonnes 'bundle' et 'is_malade'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame avec les bundles et leurs scores de précision respectifs.\n",
    "    \"\"\"\n",
    "    total = pd.DataFrame()\n",
    "    df = add_nb_patients_and_diseased(df)\n",
    "    df.loc[df['metric'] == 'precision', df.columns != 'metric'] = df.loc[df['metric'] == 'precision', df.columns != 'metric'].replace(0, np.nan)\n",
    "\n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','metric','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'metric']].copy()\n",
    "        grouped_df = bundle_df.groupby(['metric']).mean().reset_index()\n",
    "        grouped_df.set_index('metric', inplace=True)\n",
    "        total = pd.concat([total, grouped_df.T])\n",
    "        \n",
    "    return total\n",
    "# Exemple d'utilisation\n",
    "precision_df = calculate_precision_by_bundle(pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"metrics_compilation.csv\")))\n",
    "precision_df = precision_df.sort_values(by='precision', ascending=False)\n",
    "precision_df.to_csv(os.path.join(directory, \"metrics_per_bundle.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE BEST BUNDLES for properties\n",
    "def calculate_properties_by_bundle(df):\n",
    "    \"\"\"\n",
    "    Calcule le score de précision par bundle.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données avec les colonnes 'bundle' et 'is_malade'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame avec les bundles et leurs scores de précision respectifs.\n",
    "    \"\"\"\n",
    "    total = pd.DataFrame()\n",
    "    df = add_nb_patients_and_diseased(df)\n",
    "\n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','property','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'property']].copy()\n",
    "        grouped_df = bundle_df.groupby(['property']).mean().reset_index()\n",
    "        grouped_df.set_index('property', inplace=True)\n",
    "        total = pd.concat([total, grouped_df.T])\n",
    "        \n",
    "    return total\n",
    "# Exemple d'utilisation\n",
    "properties_df = calculate_properties_by_bundle(pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"properties_compilation.csv\")))\n",
    "properties_df = properties_df.sort_values(by='dists', ascending=False)\n",
    "properties_df.to_csv(os.path.join(directory, \"properties_per_bundle.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_df = properties_df.sort_values(by='dists', ascending=False)\n",
    "properties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT BUNDLES PER OUTLIERS\n",
    "def count_bundles_per_outliers(df):\n",
    "    \"\"\"\n",
    "    Analyze outliers in the DataFrame and calculate the percentage of SIDs with a certain number of occurrences.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing 'sid', 'is_outlier', and 'is_sick' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the percentage of SIDs with a certain number of occurrences for sick and healthy groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the number of occurrences of each SID\n",
    "    # Count the number of occurrences of each combination of SID and site\n",
    "    sid_counts = df.groupby(['sid', 'site', 'is_malade']).size().reset_index(name='count_bundle')\n",
    "    \n",
    "    # Divide the dataset into two groups: sick and healthy\n",
    "    sick_sids = sid_counts[sid_counts['is_malade'] == 1]\n",
    "    healthy_sids = sid_counts[sid_counts['is_malade'] == 0]\n",
    "    \n",
    "    # Calculate the percentage of SIDs with a certain number of occurrences for sick group\n",
    "    sick_counts = sick_sids.groupby(['count_bundle']).size().reset_index(name='prct_occurence')\n",
    "    sick_counts['prct_occurence'] = sick_counts['prct_occurence']/sick_counts['prct_occurence'].sum()*100\n",
    "    # Calculate the percentage of SIDs with a certain number of occurrences for healthy group\n",
    "    healthy_counts = healthy_sids.groupby(['count_bundle']).size().reset_index(name='prct_occurence')\n",
    "    healthy_counts['prct_occurence'] = healthy_counts['prct_occurence']/healthy_counts['prct_occurence'].sum()*100\n",
    "\n",
    "    total = pd.merge(sick_counts, healthy_counts, on=['count_bundle'], suffixes=('_sick', '_healthy'))\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Example usage\n",
    "bundles_per_outliers = count_bundles_per_outliers(pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"outliers_compilation.csv\")))\n",
    "bundles_per_outliers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION BOX PLOT POUR Properties\n",
    "def plot_bundle(df,property_type, nb_patient, directory):\n",
    "    \"\"\"\n",
    "    Crée un graphique pour chaque bundle dans le DataFrame donné.\n",
    "    L'axe des X représente le nombre de patients et l'axe des Y représente la moyenne de la colonne du bundle.\n",
    "    La courbe inclut une zone indiquant l'écart-type (std).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données.\n",
    "    bundle_column (str): Le nom de la colonne du bundle à utiliser pour le graphique.\n",
    "    \"\"\"\n",
    "    df = df[df['property'] == property_type]\n",
    "    directory = os.path.join(directory, \"PROPERTIES_PLOTS\",property_type,str(nb_patient))\n",
    "    df = df[df['num_patients'] == nb_patient]\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','property','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'site', 'property','num_patients','disease_ratio','num_diseased']].copy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        width = 0.2  # the width of the bars\n",
    "        x = np.arange(len(bundle_df['disease_ratio'].unique()))  # the label locations\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        data = [bundle_df[bundle_df['disease_ratio'] == patients][bundle_column].values \n",
    "            for patients in bundle_df['disease_ratio'].unique()]\n",
    "\n",
    "        # Ensure there is data for each num_patients\n",
    "        if any(len(d) > 0 for d in data):\n",
    "            ax.boxplot(data, positions=x, widths=0.15, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='blue', color='blue'),\n",
    "                   medianprops=dict(color='black'))\n",
    "                \n",
    "        ax.set_xlabel('% de malades')\n",
    "        ax.set_ylabel(property_type)\n",
    "        ax.set_title(f'Boxplots pour le bundle: {bundle_column} avec {nb_patient} patients')\n",
    "        methods = ['Method']\n",
    "        colors = ['blue']\n",
    "        ax.set_xticks(x + width * (len(methods) - 1) / 2)\n",
    "        ax.set_xticklabels(bundle_df['disease_ratio'].unique())\n",
    "        ax.legend(handles=[plt.Line2D([0], [0], color=color, lw=4, label=f'Method: {method}') for method, color in zip(methods, colors)])\n",
    "        plt.savefig(os.path.join(directory, f'{bundle_column}_boxplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "properties_df = pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"properties_compilation.csv\"))\n",
    "properties_df.loc[properties_df['property'] == 'skewness', properties_df.columns.difference(['site', 'property'])] = properties_df.loc[properties_df['property'] == 'skewness', properties_df.columns.difference(['site', 'property'])].abs()\n",
    "add_nb_patients_and_diseased(properties_df)\n",
    "properties_types = properties_df['property'].unique()\n",
    "sample_sizes = [30, 50, 100, 150, 200, 300]\n",
    "for property_type in properties_types:\n",
    "    for sample_size in sample_sizes:\n",
    "        plot_bundle(properties_df,property_type, sample_size, os.path.join(MAINFOLDER, robust_method, ANALYSISFOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TEST ADD BIAIS\n",
    "# # Split the data into training and testing sets\n",
    "# directory = os.path.join(MAINFOLDER, \"testBiais\")\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "# train_df, test_df = split_train_test(CAMCAN, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Generate biased data\n",
    "# # Save the original non-biased data to temporary files\n",
    "# temp_train_file_original = os.path.join(directory, \"temp_train_original.csv\")\n",
    "# temp_test_file_original = os.path.join(directory, \"temp_test_original.csv\")\n",
    "# train_df.to_csv(temp_train_file_original, index=False)\n",
    "# test_df.to_csv(temp_test_file_original, index=False)\n",
    "\n",
    "# # Generate biased data\n",
    "# sampled_df_biaied, test_df_biaised, gammas,deltas, ruffles= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "# # Save the biased data to temporary files\n",
    "# temp_train_file = os.path.join(directory, \"temp_train_biased.csv\")\n",
    "# temp_test_file = os.path.join(directory, \"temp_test_biased.csv\")\n",
    "# sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "# test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "# # Run the combat_visualize_data script\n",
    "# outname_train = os.path.join(\"visualize_train\")\n",
    "# cmd = (\n",
    "#     \"scripts/combat_visualize_data.py\"\n",
    "#     + \" \"\n",
    "#     + temp_train_file_original\n",
    "#     + \" \"\n",
    "#     + temp_train_file\n",
    "#     + \" --out_dir \"\n",
    "#     + directory\n",
    "#     + \" --outname \"\n",
    "#     + outname_train\n",
    "#     + \" -f\"\n",
    "#     + \" --bundles all\"\n",
    "# )\n",
    "# subprocess.call(cmd, shell=True)\n",
    "\n",
    "# # Display gammas and deltas along with their mean and standard deviation\n",
    "# print(\"Gammas:\", gammas)\n",
    "# print(\"Deltas:\", deltas)\n",
    "# gammas = list(gammas.values())\n",
    "# deltas = list(deltas.values())\n",
    "# print(\"\\nGamma Statistics:\")\n",
    "# print(f\"Mean: {np.mean(gammas)}, Std: {np.std(gammas)}\")\n",
    "\n",
    "# print(\"\\nDelta Statistics:\")\n",
    "# print(f\"Mean: {np.mean(deltas)}, Std: {np.std(deltas)}\")\n",
    "# print(\"Ruffles:\", ruffles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST Powerpoint generation\n",
    "# d  = os.path.join(MAINFOLDER, robust_method, \"adni_100_Philips_3T\")\n",
    "# create_presentation(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST the sample_patients function with compilation data data\n",
    "# sampled_df = sample_patients(COMPILATION, num_patients=100, disease_ratio=0.5)\n",
    "# print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_metrics(\"ROBUST/IQR/50_30/0/\", \"50_patients_30_percent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dists_compilation and metrics_compilation CSV files\n",
    "# dists_compilation_path = os.path.join(directory, \"dists_compilation.csv\")\n",
    "# metrics_compilation_path = os.path.join(directory, \"metrics_compilation.csv\")\n",
    "\n",
    "# dists_compilation = pd.read_csv(dists_compilation_path)\n",
    "# metrics_compilation = pd.read_csv(metrics_compilation_path)\n",
    "\n",
    "# # Change the site column\n",
    "# dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "# metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Display the means by site\n",
    "# dists_means_by_site = dists_compilation.groupby(['site','comparaison']).mean()\n",
    "# metrics_means_by_site = metrics_compilation.groupby('site').mean()\n",
    "\n",
    "# print(dists_means_by_site)\n",
    "# print(metrics_means_by_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIX METRICS COMPILATION\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# df = pd.read_csv(os.path.join(directory, \"metrics_compilation.csv\"))\n",
    "\n",
    "# # Group by the site\n",
    "# grouped = df.groupby('site')\n",
    "\n",
    "# # Process each site\n",
    "# cleaned_dfs = []\n",
    "# for site, group in grouped:\n",
    "#     # Reset index for easier manipulation\n",
    "#     group = group.reset_index(drop=True)\n",
    "    \n",
    "#     # # The first row is the \"bundle row\" (new column names)\n",
    "#     # new_columns = group.iloc[0].values  # Extract column names from the first row\n",
    "#     # new_columns[-1] = 'site'\n",
    "#     # group = group.iloc[1:]  # Remove the first row\n",
    "    \n",
    "#     # # Assign new column names\n",
    "#     # group.columns = new_columns\n",
    "    \n",
    "#     # # Sort the columns alphabetically (excluding 'site')\n",
    "#     # sorted = group.sort_index(axis=1)\n",
    "#     # Add a new column 'nomm' with the value indicating the metric for each row\n",
    "#     metrics = ['tp', 'fp', 'tn', 'fn', 'precision', 'recall', 'taux_faux_positifs', 'f1_score']\n",
    "#     group['metric'] = metrics\n",
    "    \n",
    "#     # # Append the cleaned DataFrame for this site\n",
    "#     cleaned_dfs.append(group)\n",
    "\n",
    "# # Concatenate all cleaned DataFrames\n",
    "# final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "# # Save or display the result\n",
    "# final_df.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL SITES\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# raw_directory = os.path.join(RAWFOLDER, site_group)\n",
    "# for filename in sorted(os.listdir(raw_directory)):\n",
    "#     f = os.path.join(raw_directory, filename)\n",
    "#     # checking if it is a file\n",
    "#     if os.path.isfile(f):\n",
    "#         analyse_site(f, robust_method, directory)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
