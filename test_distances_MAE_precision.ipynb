{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS and UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "import os\n",
    "\n",
    "from scripts import combat_info\n",
    "from scripts import combat_quick_apply\n",
    "from scripts import combat_quick_QC\n",
    "\n",
    "\n",
    "CAMCAN = \"./DONNES/CamCAN.md.raw.csv.gz\"\n",
    "COMPILATION = \"./DONNES/adni_compilation.csv.gz\"\n",
    "\n",
    "SYNTHETIC_SITES = \"ROBUST/SYNTHETIC_SITES\"\n",
    "\n",
    "MAINFOLDER = \"ROBUST\"\n",
    "\n",
    "RAWFOLDER = \"RAW\"\n",
    "\n",
    "ANALYSISFOLDER = \"ANALYSIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(mov_data_file):\n",
    "    [df,bundles] = combat_info.info(mov_data_file)\n",
    "    nb_hc = int(re.findall('HC\\(n=(\\d+)',df[\"DetailInfos\"][\"Disease\"])[0])\n",
    "    nb_total = df[\"DetailInfos\"][\"Number of Subject\"]\n",
    "    nb_sick = nb_total - nb_hc\n",
    "    return [nb_total,nb_hc,nb_sick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundles(mov_data_file):\n",
    "    return combat_info.get_bundles(mov_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_text(x):\n",
    "    return \"NoRobust\" if x == 'No' else x\n",
    "\n",
    "def rwp_text(x):\n",
    "    return \"RWP\" if x else \"NoRWP\"\n",
    "def get_site(mov_data_file):\n",
    "    mov_data = pd.read_csv(mov_data_file)\n",
    "    return mov_data.site.unique()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nb_patients_and_diseased(df):\n",
    "  df['num_patients'] = df['site'].str.extract(r'(\\d+)_patients')[0].astype(int)\n",
    "  df['disease_ratio'] = df['site'].str.extract(r'(\\d+)_percent')[0].astype(int)\n",
    "  df['num_diseased'] = (df['num_patients'] * df['disease_ratio']/100).astype(int)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df1,df2, title, bundle='mni_MCP'):\n",
    "    df1_bundle = df1[df1['bundle'] == bundle]\n",
    "    df2_bundle = df2[df2['bundle'] == bundle]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(df1_bundle['age'], df1_bundle['mean'], label='Train', alpha=0.5, color='green')\n",
    "    plt.scatter(df2_bundle['age'], df2_bundle['mean'], label='Test', alpha=0.5, color='red')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SITE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(file_path, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into training and testing sets, ensuring the same proportion of HC and non-HC patients\n",
    "    and that data from the same sid are in the same dataset.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to split.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Training set.\n",
    "    pd.DataFrame: Testing set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Group by 'sid' and get unique sids\n",
    "    unique_sids = df.groupby('sid').first().reset_index()\n",
    "    \n",
    "    # Split the unique sids into train and test sets\n",
    "    train_sids, test_sids = train_test_split(unique_sids, test_size=test_size, random_state=random_state, stratify=unique_sids['disease'])\n",
    "    \n",
    "    # Create train and test DataFrames by filtering the original DataFrame\n",
    "    train_df = df[df['sid'].isin(train_sids['sid'])]\n",
    "    test_df = df[df['sid'].isin(test_sids['sid'])]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_patients(df, num_patients, disease_ratio,index):\n",
    "    # Lire le fichier CSV dans un DataFrame\n",
    "    \n",
    "    # Calculer le nombre de patients malades et sains\n",
    "    num_diseased = int(num_patients * disease_ratio)\n",
    "    num_healthy = num_patients - num_diseased\n",
    "    \n",
    "    # Filtrer les patients en santé (HC) et malades\n",
    "    healthy_patients = df[df['disease'] == 'HC']\n",
    "    diseased_patients = df[df['disease'] != 'HC']\n",
    "    \n",
    "    # S'assurer qu'il y a assez de patients pour chaque catégorie\n",
    "    if len(healthy_patients['sid'].unique()) < num_healthy or len(diseased_patients['sid'].unique()) < num_diseased:\n",
    "        raise ValueError(\"Nombre insuffisant de patients en santé ou malades pour l'échantillon demandé.\")\n",
    "    \n",
    "    # Sélectionner un échantillon aléatoire de patients sains et malades\n",
    "    sampled_healthy = healthy_patients.groupby('sid').sample(frac=1).head(num_healthy * df['bundle'].nunique())\n",
    "    sampled_diseased = diseased_patients.groupby('sid').sample(frac=1).head(num_diseased * df['bundle'].nunique())\n",
    "    \n",
    "    # Combiner les échantillons pour obtenir le DataFrame final\n",
    "    sampled_df = pd.concat([sampled_healthy, sampled_diseased])\n",
    "    # Modifier les valeurs de 'site' pour toutes les lignes\n",
    "    sampled_df['site'] = f\"{num_patients}_patients_{int(disease_ratio*100)}_percent_{index}\"\n",
    "    \n",
    "    # Retourner le DataFrame final\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biaised_data(df1, df2, \n",
    "                additive_uniform_low=-3, additive_uniform_high=3, \n",
    "                multiplicative_uniform_low=0.5, multiplicative_uniform_high=2, \n",
    "                additive_std_low=0.01, additive_std_high=0.1, \n",
    "                multiplicative_std_low=0.01, multiplicative_std_high=0.1):\n",
    "    \"\"\"\n",
    "    Génère des biais additifs et multiplicatifs pour chaque bundle en fonction de df1, puis applique ces biais à df1 et df2\n",
    "    de manière indépendante en tenant compte des covariables (âge, sexe, latéralité) et en centrant les résidus.\n",
    "\n",
    "    Parameters:\n",
    "    - df1, df2 (pd.DataFrame): Les DataFrames sur lesquels appliquer les biais.\n",
    "    - additive_uniform_low, additive_uniform_high : paramètres pour le biais additif.\n",
    "    - multiplicative_uniform_low, multiplicative_uniform_high : paramètres pour le biais multiplicatif.\n",
    "    - additive_std_low, additive_std_high : paramètres pour l'écart-type du biais additif.\n",
    "    - multiplicative_std_low, multiplicative_std_high : paramètres pour l'écart-type du biais multiplicatif.\n",
    "\n",
    "    Returns:\n",
    "    - tuple : Deux DataFrames avec les biais appliqués indépendamment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionnaires pour stocker les biais par bundle\n",
    "    additive_bias_per_bundle = {}\n",
    "    multiplicative_bias_per_bundle = {}\n",
    "\n",
    "    # # Tirer les moyennes de biais de distributions uniformes pour le bundle\n",
    "    additive_mean = np.random.uniform(low=additive_uniform_low, high=additive_uniform_high)\n",
    "    multiplicative_mean = np.random.uniform(low=multiplicative_uniform_low, high=multiplicative_uniform_high)\n",
    "    \n",
    "    # # Tirer les écarts-types de biais de distributions uniformes pour le bundle\n",
    "    additive_std = np.random.uniform(low=additive_std_low, high=additive_std_high)\n",
    "    multiplicative_std = np.random.uniform(low=multiplicative_std_low, high=multiplicative_std_high)\n",
    "\n",
    "    # Calcul des biais pour chaque bundle unique dans df1\n",
    "    for bundle in df1['bundle'].unique(): \n",
    "        # Générer un biais additif et multiplicatif spécifique au bundle\n",
    "        additive_bias_per_bundle[bundle] = np.random.normal(loc=additive_mean, scale=additive_std)\n",
    "        multiplicative_bias_per_bundle[bundle] = np.random.normal(loc=multiplicative_mean, scale=multiplicative_std)\n",
    "   \n",
    "    # Appliquer les biais indépendamment à df1 et df2 en utilisant les mêmes biais générés\n",
    "    combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    biased_df = apply_bias(combined, additive_bias_per_bundle, multiplicative_bias_per_bundle)\n",
    "    biased_df1 = biased_df[biased_df['sid'].isin(df1['sid'])]\n",
    "    biased_df2 = biased_df[biased_df['sid'].isin(df2['sid'])]\n",
    "    bias_parameters = {\n",
    "        'additive_mean': additive_mean,\n",
    "        'multiplicative_mean': multiplicative_mean,\n",
    "        'additive_std': additive_std,\n",
    "        'multiplicative_std': multiplicative_std\n",
    "    }\n",
    "    \n",
    "    return biased_df1, biased_df2, additive_bias_per_bundle, multiplicative_bias_per_bundle, bias_parameters\n",
    "\n",
    "def apply_bias(dataframe, additive_bias_per_bundle, multiplicative_bias_per_bundle):\n",
    "    biased_df = dataframe.copy()\n",
    "    \n",
    "    # Application de la régression et des biais pour chaque bundle unique\n",
    "    for bundle in biased_df['bundle'].unique():\n",
    "        # Filtrer le DataFrame pour le bundle actuel\n",
    "        bundle_df = biased_df[biased_df['bundle'] == bundle]\n",
    "\n",
    "        # Préparer les covariables pour la régression\n",
    "        X = bundle_df[['age', 'sex', 'handedness']]\n",
    "        y = bundle_df['mean']\n",
    "        \n",
    "        # Ajuster le modèle de régression linéaire pour le bundle\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Calculer les prédictions et les résidus pour le bundle\n",
    "        predicted_mean = model.predict(X)\n",
    "        residuals = y - predicted_mean\n",
    "\n",
    "        # Récupérer les biais pour le bundle actuel\n",
    "        additive_bias = additive_bias_per_bundle[bundle]\n",
    "        multiplicative_bias = multiplicative_bias_per_bundle[bundle]\n",
    "        \n",
    "        # Appliquer les biais aux résidus centrés et réintégrer les effets des covariables\n",
    "        biased_means_bundle = residuals * multiplicative_bias + additive_bias * np.std(residuals) + predicted_mean\n",
    "        biased_df.loc[biased_df['bundle'] == bundle, 'mean'] = biased_means_bundle\n",
    "    \n",
    "    # Assigner les valeurs biaisées calculées au DataFrame\n",
    "    return biased_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATE SITES\n",
    "def generate_sites(sample_sizes, disease_ratios, num_tests, SYNTHETIC_SITES_VERSION):\n",
    "    directory = os.path.join(SYNTHETIC_SITES, SYNTHETIC_SITES_VERSION)\n",
    "    train_df, test_df = split_train_test(COMPILATION, test_size=0.2, random_state=42)\n",
    "    # Initialize DataFrames to store the results\n",
    "    for sample_size in sample_sizes:\n",
    "        for disease_ratio in disease_ratios:  \n",
    "            sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            for i in range(num_tests):\n",
    "                \n",
    "                tempDir = os.path.join(sizeDir, f\"{i}\")\n",
    "                os.makedirs(tempDir, exist_ok=True)\n",
    "\n",
    "                train_df_biaised, test_df_biaised, gammas, deltas, parameters= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "                sampled_df_biaied =  sample_patients(train_df_biaised, sample_size, disease_ratio,i)\n",
    "\n",
    "                # Sauvegarder l'échantillon dans un fichier temporaire\n",
    "                temp_train_file = os.path.join(tempDir, f\"train_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "                sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "                \n",
    "                temp_test_file = os.path.join(tempDir, f\"test_{sample_size}_{int(disease_ratio*100)}_{i}.csv\")\n",
    "                test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "                # Sauvegarde dans un fichier JSON\n",
    "                with open(os.path.join(tempDir,'parameters.json'), 'w') as file:\n",
    "                    json.dump({'parameters': parameters, 'gammas': gammas, 'deltas': deltas}, file, indent=4)\n",
    "\n",
    "                cmd = (\n",
    "                    \"scripts/combat_visualize_data.py\"\n",
    "                    + \" \"\n",
    "                    + COMPILATION\n",
    "                    + \" \"\n",
    "                    + temp_train_file\n",
    "                    + \" --out_dir \"\n",
    "                    + os.path.join(tempDir, \"VIZ\")\n",
    "                    + \" -f\"\n",
    "                    + \" --bundles all\"\n",
    "                )\n",
    "                subprocess.call(cmd, shell=True)\n",
    "                cmd = (\n",
    "                    \"scripts/combat_visualize_data.py\"\n",
    "                    + \" \"\n",
    "                    + COMPILATION\n",
    "                    + \" \"\n",
    "                    + temp_test_file\n",
    "                    + \" --out_dir \"\n",
    "                    + os.path.join(tempDir, \"VIZ_TEST\")\n",
    "                    + \" -f\"\n",
    "                    + \" --bundles all\"\n",
    "                )\n",
    "                subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARMONIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(mov_data_file, robust, rwp, directory, hc,):\n",
    "    ###########\n",
    "    ### fit ###\n",
    "    ###########\n",
    "    output_model_filename = (\n",
    "            get_site(mov_data_file)\n",
    "            + \".\"\n",
    "            + metric\n",
    "            + \".\"\n",
    "            + method\n",
    "            + \".\"\n",
    "            + robust_text(robust)\n",
    "            + \".\"\n",
    "            + rwp_text(rwp)\n",
    "            + \".model.csv\"\n",
    "        )\n",
    "    cmd = (\n",
    "        \"scripts/combat_quick_fit.py\"\n",
    "        + \" \"\n",
    "        + CAMCAN\n",
    "        + \" \"\n",
    "        + mov_data_file\n",
    "        + \" --out_dir \"\n",
    "        + directory\n",
    "        + \" --output_model_filename \"\n",
    "        + output_model_filename\n",
    "        + \" --method \"\n",
    "        + method\n",
    "        + \" --robust \"\n",
    "        + robust\n",
    "        + \" -f \"\n",
    "    )\n",
    "    if rwp:\n",
    "        cmd += ' --rwp'\n",
    "    if hc: \n",
    "        cmd += ' --hc'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    return output_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(mov_data_file, model_filename, robust, rwp, directory):\n",
    "    output_filename = os.path.join(\n",
    "            directory,\n",
    "            get_site(mov_data_file)\n",
    "            + \".\"\n",
    "            + metric\n",
    "            + \".\"\n",
    "            + method\n",
    "            + \".\"\n",
    "            + robust_text(robust)\n",
    "            + \".\"\n",
    "            + rwp_text(rwp)\n",
    "            + \".csv\"\n",
    "        )\n",
    "    combat_quick_apply.apply(mov_data_file, model_filename, output_filename)\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_harmonization(f, new_f, directory):\n",
    "    cmd = (\n",
    "        \"scripts/combat_visualize_harmonization.py\"\n",
    "        + \" \"\n",
    "        + CAMCAN\n",
    "        + \" \"\n",
    "        + f\n",
    "        + \" \"\n",
    "        + new_f\n",
    "        + \" --out_dir \"\n",
    "        + directory\n",
    "        #+ \" --bundles all\"\n",
    "        + \" -f\"\n",
    "    )\n",
    "    subprocess.call(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC(output_filename, output_model_filename):\n",
    "    return combat_quick_QC.QC(CAMCAN,output_filename, output_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_compilation(df):\n",
    "    # Charger le DataFrame COMPILATION\n",
    "    compilation_df = pd.read_csv(COMPILATION)\n",
    "    \n",
    "    # Filtrer les patients de COMPILATION qui sont dans df en utilisant les sid\n",
    "    common_sids = df['sid'].unique()\n",
    "    filtered_compilation_df = compilation_df[compilation_df['sid'].isin(common_sids)]\n",
    "    \n",
    "    # Initialiser une liste pour stocker les résultats\n",
    "    comparison_df = pd.DataFrame()\n",
    "\n",
    "    # Comparer la différence absolue de la colonne mean par bundle\n",
    "    for bundle in df['bundle'].unique():\n",
    "        df_bundle = df[df['bundle'] == bundle]\n",
    "        compilation_bundle = filtered_compilation_df[filtered_compilation_df['bundle'] == bundle]\n",
    "        \n",
    "        # Fusionner les deux DataFrames sur les colonnes 'sid' et 'bundle'\n",
    "        merged_df = pd.merge(df_bundle, compilation_bundle, on=['sid', 'bundle'], suffixes=('_df', '_compilation'))\n",
    "        \n",
    "        # Calculer la différence absolue de la colonne mean\n",
    "        merged_df['abs_diff_mean'] = (merged_df['mean_df'] - merged_df['mean_compilation']).abs()\n",
    "        # Calculer la somme des différences absolues pour le bundle\n",
    "        comparison_df[bundle] = merged_df['abs_diff_mean']\n",
    "           \n",
    "    # Ajouter le site au DataFrame\n",
    "    mean_df = pd.DataFrame(comparison_df.mean()).transpose()\n",
    "    \n",
    "    return mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_presentation(directory):\n",
    "    # Create a presentation object\n",
    "    prs = Presentation()\n",
    "    \n",
    "    # Define the subdirectories\n",
    "    subdirs = [\"hc\", \"NoRobust\", \"robust\", \"robust_rwp\"]\n",
    "    # Get the list of images\n",
    "    images = [img for img in os.listdir(os.path.join(directory, subdirs[0])) if method in img and img.endswith('.png')]\n",
    "    \n",
    "    for img in images:\n",
    "        slide_layout = prs.slide_layouts[5]  # Use a blank slide layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        \n",
    "        for i, subdir in enumerate(subdirs):\n",
    "            img_path = os.path.join(directory, subdir, img)\n",
    "            left = Inches(0.5 + (i % 2) * 4.5)  # Positioning images in two columns\n",
    "            top = Inches(0.2 + (i // 2) * 3.5)  # Positioning images in two rows with more space between rows\n",
    "            \n",
    "            # Add text above the image\n",
    "            text_box = slide.shapes.add_textbox(left, top, width=Inches(4), height=Inches(0.5))\n",
    "            text_frame = text_box.text_frame\n",
    "            text_frame.text = subdir\n",
    "            \n",
    "            # Add the image\n",
    "            slide.shapes.add_picture(img_path, left, top + Inches(0.5), width=Inches(4))\n",
    "    \n",
    "    # Save the presentation\n",
    "    prs.save(os.path.join(directory, 'harmonization_results.pptx'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distances(directory, site, hc_dists, no_robust_dists, robust_dists, robust_rwp_dists):\n",
    "    comparison_results = {\n",
    "        \"hc_vs_no_robust\": (np.array(hc_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100,\n",
    "        \"robust_vs_no_robust\": (np.array(robust_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100,\n",
    "        \"robust_rwp_vs_no_robust\": (np.array(robust_rwp_dists) - np.array(no_robust_dists))/np.array(no_robust_dists)*100\n",
    "    }\n",
    "    df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Calculer le nombre de comparaisons négatives et positives, et les moyennes et médianes\n",
    "    results = []\n",
    "    for method in comparison_results.keys():\n",
    "        negative_values = df[method][df[method] < 0]\n",
    "        positive_values = df[method][df[method] >= 0]\n",
    "        \n",
    "        num_negative = len(negative_values)\n",
    "        num_positive = len(positive_values)\n",
    "        \n",
    "        mean_negative = negative_values.mean() if num_negative > 0 else 0\n",
    "        mean_positive = positive_values.mean() if num_positive > 0 else 0\n",
    "        \n",
    "        median_negative = negative_values.median() if num_negative > 0 else 0\n",
    "        median_positive = positive_values.median() if num_positive > 0 else 0\n",
    "        \n",
    "        mean_difference = df[method].mean()\n",
    "        \n",
    "        results.append({\n",
    "            \"site\": site,\n",
    "            \"comparaison\": method,\n",
    "            \"Nb comp. nég.\": num_negative,\n",
    "            \"Nb comp. pos.\": num_positive,\n",
    "            \"Moy. tot.\": mean_difference,\n",
    "            \"Moy. val. nég.\": mean_negative,\n",
    "            \"Moy. val. pos.\": mean_positive,\n",
    "            \"Méd. val. nég.\": median_negative,\n",
    "            \"Méd. val. pos.\": median_positive\n",
    "        })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(directory, f\"{site}_comparison_results.csv\"), index=False)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize(f_train, f_test, directory, robust, rwp,hc):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f_train)\n",
    "    \n",
    "    # Fit the model\n",
    "    output_model_filename = fit(f_train, robust, rwp, directory, hc)\n",
    "    output_model_filename = os.path.join(directory, output_model_filename)\n",
    "    # Apply the model\n",
    "    output_filename = apply(f_test, output_model_filename, robust, rwp, directory) \n",
    "    \n",
    "    # Perform quality control\n",
    "    dists, bundle_names = QC(output_filename, output_model_filename)\n",
    "    dists_df = pd.DataFrame([dists], columns=bundle_names)\n",
    "    dists_df['site'] = get_site(f_train)\n",
    "    \n",
    "    # Visualize the harmonization\n",
    "    visualize_harmonization(f_test, output_filename, directory)\n",
    "\n",
    "    mea = compare_with_compilation(pd.read_csv(output_filename))\n",
    "    mea['site'] = get_site(f_train)\n",
    "    \n",
    "    # If robust is not \"No\", load metrics and outliers\n",
    "    if robust != \"No\":\n",
    "        metrics_filename = os.path.join(directory, f\"metrics_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "        outliers_filename = os.path.join(directory, f\"outliers_{get_site(f_train)}_{robust_text(robust)}_{rwp_text(rwp)}.csv\")\n",
    "        \n",
    "        # Load metrics from CSV file\n",
    "        loaded_metrics = pd.read_csv(metrics_filename, index_col=0)\n",
    "        \n",
    "        # Load outliers from CSV file\n",
    "        loaded_outliers_df = pd.read_csv(outliers_filename, index_col=0)\n",
    "        \n",
    "        return [dists_df, mea, loaded_metrics, loaded_outliers_df]\n",
    "    return[dists_df, mea, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_site(f_train,f_test, robust, directory):\n",
    "    # 4 harmonization\n",
    "    harmonization_hc = harmonize(f_train, f_test, os.path.join(directory, \"hc\"), \"No\", False, True)\n",
    "    harmonization_no_robust = harmonize(f_train, f_test, os.path.join(directory, \"NoRobust\"), \"No\", False, False)\n",
    "    harmonization_robust = harmonize(f_train, f_test, os.path.join(directory, \"robust\"), robust, False, False)\n",
    "    harmonization_robust_rwp = harmonize(f_train, f_test, os.path.join(directory, \"robust_rwp\"), robust, True, False)\n",
    "\n",
    "\n",
    "    create_presentation(directory)\n",
    "\n",
    "    #dists_analyze = compare_distances(directory, get_site(f_train), harmonization_hc[0], harmonization_no_robust[0], harmonization_robust[0], harmonization_robust_rwp[0])\n",
    "    # Combine distances in a single DataFrame\n",
    "    distances_combined = pd.concat([harmonization_hc[0], harmonization_no_robust[0], harmonization_robust[0], harmonization_robust_rwp[0]], ignore_index=True)\n",
    "    distances_combined['method'] = ['hc', 'no_robust', 'robust', 'robust_rwp']\n",
    "\n",
    "    # Combine MEA in a single DataFrame\n",
    "    mea_combined = pd.concat([harmonization_hc[1], harmonization_no_robust[1], harmonization_robust[1], harmonization_robust_rwp[1]], ignore_index=True)\n",
    "    mea_combined['method'] = ['hc', 'no_robust', 'robust', 'robust_rwp']\n",
    "\n",
    "\n",
    "    #TODO bundles et analyze outliers\n",
    "    return distances_combined, mea_combined, harmonization_robust[2], harmonization_robust[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Analyse Method\n",
    "def analyse_method(sample_sizes, disease_ratios, num_tests, robust_method, SYNTHETIC_SITES_VERSION):\n",
    "    # Split the data into training and testing sets\n",
    "    directory = os.path.join(MAINFOLDER, robust_method)\n",
    "    directory_site = os.path.join(SYNTHETIC_SITES ,SYNTHETIC_SITES_VERSION)\n",
    "    # Initialize DataFrames to store the results\n",
    "    metrics_compilation = pd.DataFrame()\n",
    "    dists_compilation = pd.DataFrame()\n",
    "    mea_compilation = pd.DataFrame()\n",
    "    outliers_compilation = pd.DataFrame()\n",
    "    for sample_size in sample_sizes:\n",
    "        for disease_ratio in disease_ratios:        \n",
    "            sizeDir = os.path.join(directory, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            sizeDir_site = os.path.join(directory_site, f\"{sample_size}_{int(disease_ratio*100)}\")\n",
    "            for i in range(num_tests):\n",
    "                tempDir = os.path.join(sizeDir, f\"{i}\")\n",
    "                tempDir_site = os.path.join(sizeDir_site, f\"{i}\")\n",
    "                os.makedirs(tempDir, exist_ok=True)\n",
    "\n",
    "                train_file_name = f\"train_{sample_size}_{int(disease_ratio*100)}_{i}.csv\"\n",
    "                test_file_name = f\"test_{sample_size}_{int(disease_ratio*100)}_{i}.csv\"\n",
    "                \n",
    "                # Sauvegarder l'échantillon dans un fichier temporaire\n",
    "                temp_file = os.path.join(tempDir_site,train_file_name )\n",
    "                train_df = pd.read_csv(temp_file)\n",
    "                train_df.to_csv(os.path.join(tempDir,train_file_name ), index=False)\n",
    "\n",
    "                test_file = os.path.join(tempDir_site, test_file_name)\n",
    "                test_df = pd.read_csv(test_file)\n",
    "                test_df.to_csv(os.path.join(tempDir,test_file_name ), index=False)\n",
    "\n",
    "                \n",
    "                # Analyser le site pour le nouvel échantillon\n",
    "                dists_analyze, mea_analyze, metrics, outliers = analyse_site(temp_file, test_file, robust_method, tempDir)\n",
    "                metrics_compilation = pd.concat([metrics_compilation, metrics])\n",
    "                dists_compilation = pd.concat([dists_compilation, dists_analyze])\n",
    "                mea_compilation = pd.concat([mea_compilation, mea_analyze])\n",
    "                outliers_compilation = pd.concat([outliers_compilation, outliers])\n",
    "    # Save the metrics and distances compilation DataFrames to CSV files\n",
    "    metrics_compilation.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n",
    "    dists_compilation.to_csv(os.path.join(directory, \"dists_compilation.csv\"), index=False)\n",
    "    mea_compilation.to_csv(os.path.join(directory, \"mea_compilation.csv\"), index=False)\n",
    "    outliers_compilation.to_csv(os.path.join(directory, \"outliers_compilation.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_group = 'ADNI'\n",
    "robust_method = 'IQR'\n",
    "metric = \"md\"\n",
    "method= \"classic\"\n",
    "\n",
    "SYNTHETIC_SITES_VERSION = \"v1\"\n",
    "\n",
    "\n",
    "sample_sizes = [30, 50, 100, 150, 200,300]  # Différentes tailles d'échantillon\n",
    "disease_ratios = [0.1, 0.3, 0.5, 0.7]  # Différents pourcentages de malades\n",
    "#sample_sizes = [150, 300]  # Différentes tailles d'échantillon\n",
    "#disease_ratios = [0.3]  # Différents pourcentages de malades\n",
    "num_tests = 10  # Nombre de tests à effectuer pour chaque combinaison\n",
    "\n",
    "#generate_sites(sample_sizes, disease_ratios, num_tests, SYNTHETIC_SITES_VERSION)\n",
    "\n",
    "#analyse_method(sample_sizes, disease_ratios, num_tests, robust_method, SYNTHETIC_SITES_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINI\n"
     ]
    }
   ],
   "source": [
    "# Moyenne par site\n",
    "# Nothing really interesting so far\n",
    "directory = os.path.join(MAINFOLDER, robust_method)\n",
    "dists_compilation = pd.read_csv(os.path.join(directory, \"dists_compilation.csv\"))\n",
    "metrics_compilation = pd.read_csv(os.path.join(directory, \"metrics_compilation.csv\"))\n",
    "directory = os.path.join(directory, ANALYSISFOLDER)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Display the means by site\n",
    "dists_means_by_site = dists_compilation.groupby(['site','method']).mean().reset_index()\n",
    "metrics_means_by_site = metrics_compilation.groupby(['site', 'metric']).mean().reset_index()\n",
    "\n",
    "metrics_means_by_site.to_csv(os.path.join(directory, \"metrics_compilation_mean.csv\"), index=False)\n",
    "dists_means_by_site.to_csv(os.path.join(directory, \"dists_compilation_mean.csv\"), index=False)\n",
    "print(\"FINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE BEST BUNDLES for F1, precision etc\n",
    "def calculate_precision_by_bundle(df):\n",
    "    \"\"\"\n",
    "    Calcule le score de précision par bundle.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données avec les colonnes 'bundle' et 'is_malade'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame avec les bundles et leurs scores de précision respectifs.\n",
    "    \"\"\"\n",
    "    total = pd.DataFrame()\n",
    "    df = add_nb_patients_and_diseased(df)\n",
    "\n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','metric','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'metric']].copy()\n",
    "        grouped_df = bundle_df.groupby(['metric']).mean().reset_index()\n",
    "        grouped_df.set_index('metric', inplace=True)\n",
    "        total = pd.concat([total, grouped_df.T])\n",
    "        \n",
    "    return total\n",
    "# Exemple d'utilisation\n",
    "precision_df = calculate_precision_by_bundle(pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"metrics_compilation.csv\")))\n",
    "precision_df = precision_df.sort_values(by='precision', ascending=False)\n",
    "precision_df.to_csv(os.path.join(directory, \"metrics_per_bundle.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_df = precision_df.sort_values(by='precision', ascending=False)\n",
    "precision_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_df = precision_df.sort_values(by='f1_score', ascending=False)\n",
    "precision_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT BUNDLES PER OUTLIERS\n",
    "def count_bundles_per_outliers(df):\n",
    "    \"\"\"\n",
    "    Analyze outliers in the DataFrame and calculate the percentage of SIDs with a certain number of occurrences.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing 'sid', 'is_outlier', and 'is_sick' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the percentage of SIDs with a certain number of occurrences for sick and healthy groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the number of occurrences of each SID\n",
    "    # Count the number of occurrences of each combination of SID and site\n",
    "    sid_counts = df.groupby(['sid', 'site', 'is_malade']).size().reset_index(name='count_bundle')\n",
    "    \n",
    "    # Divide the dataset into two groups: sick and healthy\n",
    "    sick_sids = sid_counts[sid_counts['is_malade'] == 1]\n",
    "    healthy_sids = sid_counts[sid_counts['is_malade'] == 0]\n",
    "    \n",
    "    # Calculate the percentage of SIDs with a certain number of occurrences for sick group\n",
    "    sick_counts = sick_sids.groupby(['count_bundle']).size().reset_index(name='prct_occurence')\n",
    "    sick_counts['prct_occurence'] = sick_counts['prct_occurence']/sick_counts['prct_occurence'].sum()*100\n",
    "    # Calculate the percentage of SIDs with a certain number of occurrences for healthy group\n",
    "    healthy_counts = healthy_sids.groupby(['count_bundle']).size().reset_index(name='prct_occurence')\n",
    "    healthy_counts['prct_occurence'] = healthy_counts['prct_occurence']/healthy_counts['prct_occurence'].sum()*100\n",
    "\n",
    "    total = pd.merge(sick_counts, healthy_counts, on=['count_bundle'], suffixes=('_sick', '_healthy'))\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Example usage\n",
    "bundles_per_outliers = count_bundles_per_outliers(pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"outliers_compilation.csv\")))\n",
    "bundles_per_outliers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION BOX PLOT POUR DISTANCES\n",
    "def plot_bundle(df, prct, directory):\n",
    "    \"\"\"\n",
    "    Crée un graphique pour chaque bundle dans le DataFrame donné.\n",
    "    L'axe des X représente le nombre de patients et l'axe des Y représente la moyenne de la colonne du bundle.\n",
    "    La courbe inclut une zone indiquant l'écart-type (std).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données.\n",
    "    bundle_column (str): Le nom de la colonne du bundle à utiliser pour le graphique.\n",
    "    \"\"\"\n",
    "    directory = os.path.join(directory, \"DISTANCES_PLOTS\", str(prct))\n",
    "    df = df[df['disease_ratio'] == prct *100]\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','method','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'site', 'method','num_patients','disease_ratio','num_diseased']].copy()\n",
    "        methods = [\"hc\", \"no_robust\", \"robust\", \"robust_rwp\"]\n",
    "        colors = ['blue', 'green', 'red', 'purple']\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        width = 0.2  # the width of the bars\n",
    "        x = np.arange(len(bundle_df['num_patients'].unique()))  # the label locations\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "            method_df = bundle_df[bundle_df['method'] == method]\n",
    "            data = [method_df[method_df['num_patients'] == patients][bundle_column].values \n",
    "                    for patients in bundle_df['num_patients'].unique()]\n",
    "            \n",
    "            # Ensure there is data for each num_patients\n",
    "            if any(len(d) > 0 for d in data):\n",
    "                positions = x + i * width  # Shift positions for each method\n",
    "                ax.boxplot(data, positions=positions, widths=0.15, patch_artist=True, \n",
    "                        boxprops=dict(facecolor=color, color=color),\n",
    "                        medianprops=dict(color='black'))\n",
    "                \n",
    "        ax.set_xlabel('Nombre de patients')\n",
    "        ax.set_ylabel('Valeurs')\n",
    "        ax.set_title(f'Boxplots pour le bundle: {bundle_column} avec {prct * 100}% de malades')\n",
    "        ax.set_xticks(x + width * (len(methods) - 1) / 2)\n",
    "        ax.set_xticklabels(bundle_df['num_patients'].unique())\n",
    "        ax.legend(handles=[plt.Line2D([0], [0], color=color, lw=4, label=f'Method: {method}') for method, color in zip(methods, colors)])\n",
    "        plt.savefig(os.path.join(directory, f'{bundle_column}_boxplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "distances_df = pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"dists_compilation.csv\"))\n",
    "add_nb_patients_and_diseased(distances_df)\n",
    "disease_ratios = [0.1, 0.3, 0.5, 0.7]\n",
    "for disease_ratio in disease_ratios:\n",
    "    plot_bundle(distances_df, disease_ratio, os.path.join(MAINFOLDER, robust_method, ANALYSISFOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION BOX PLOT POUR MEA\n",
    "def plot_bundle(df, prct, directory):\n",
    "    \"\"\"\n",
    "    Crée un graphique pour chaque bundle dans le DataFrame donné.\n",
    "    L'axe des X représente le nombre de patients et l'axe des Y représente la moyenne de la colonne du bundle.\n",
    "    La courbe inclut une zone indiquant l'écart-type (std).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Le DataFrame contenant les données.\n",
    "    bundle_column (str): Le nom de la colonne du bundle à utiliser pour le graphique.\n",
    "    \"\"\"\n",
    "    directory = os.path.join(directory, \"MEA_PLOTS\", str(prct))\n",
    "    df = df[df['disease_ratio'] == prct *100]\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    for bundle_column in df.columns:\n",
    "        if bundle_column in ['site','method','num_patients','disease_ratio','num_diseased']:\n",
    "            continue # Skip non-numeric columns\n",
    "        bundle_df = df[[bundle_column, 'site', 'method','num_patients','disease_ratio','num_diseased']].copy()\n",
    "        methods = [\"hc\", \"no_robust\", \"robust\", \"robust_rwp\"]\n",
    "        colors = ['blue', 'green', 'red', 'purple']\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        width = 0.2  # the width of the bars\n",
    "        x = np.arange(len(bundle_df['num_patients'].unique()))  # the label locations\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "            method_df = bundle_df[bundle_df['method'] == method]\n",
    "            data = [method_df[method_df['num_patients'] == patients][bundle_column].values \n",
    "                    for patients in bundle_df['num_patients'].unique()]\n",
    "            \n",
    "            # Ensure there is data for each num_patients\n",
    "            if any(len(d) > 0 for d in data):\n",
    "                positions = x + i * width  # Shift positions for each method\n",
    "                ax.boxplot(data, positions=positions, widths=0.15, patch_artist=True, \n",
    "                        boxprops=dict(facecolor=color, color=color),\n",
    "                        medianprops=dict(color='black'))\n",
    "                \n",
    "        ax.set_xlabel('Nombre de patients')\n",
    "        ax.set_ylabel('Valeurs')\n",
    "        ax.set_title(f'Boxplots pour le bundle: {bundle_column} avec {prct * 100}% de malades')\n",
    "        ax.set_xticks(x + width * (len(methods) - 1) / 2)\n",
    "        ax.set_xticklabels(bundle_df['num_patients'].unique())\n",
    "        ax.legend(handles=[plt.Line2D([0], [0], color=color, lw=4, label=f'Method: {method}') for method, color in zip(methods, colors)])\n",
    "        plt.savefig(os.path.join(directory, f'{bundle_column}_boxplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "mea_df = pd.read_csv(os.path.join(MAINFOLDER, robust_method, \"mea_compilation.csv\"))\n",
    "add_nb_patients_and_diseased(mea_df)\n",
    "disease_ratios = [0.1, 0.3, 0.5, 0.7]\n",
    "for disease_ratio in disease_ratios:\n",
    "    plot_bundle(mea_df, disease_ratio, os.path.join(MAINFOLDER, robust_method, ANALYSISFOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TEST ADD BIAIS\n",
    "# # Split the data into training and testing sets\n",
    "# directory = os.path.join(MAINFOLDER, \"testBiais\")\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "# train_df, test_df = split_train_test(CAMCAN, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Generate biased data\n",
    "# # Save the original non-biased data to temporary files\n",
    "# temp_train_file_original = os.path.join(directory, \"temp_train_original.csv\")\n",
    "# temp_test_file_original = os.path.join(directory, \"temp_test_original.csv\")\n",
    "# train_df.to_csv(temp_train_file_original, index=False)\n",
    "# test_df.to_csv(temp_test_file_original, index=False)\n",
    "\n",
    "# # Generate biased data\n",
    "# sampled_df_biaied, test_df_biaised, gammas,deltas, ruffles= generate_biaised_data(train_df, test_df)\n",
    "\n",
    "# # Save the biased data to temporary files\n",
    "# temp_train_file = os.path.join(directory, \"temp_train_biased.csv\")\n",
    "# temp_test_file = os.path.join(directory, \"temp_test_biased.csv\")\n",
    "# sampled_df_biaied.to_csv(temp_train_file, index=False)\n",
    "# test_df_biaised.to_csv(temp_test_file, index=False)\n",
    "\n",
    "# # Run the combat_visualize_data script\n",
    "# outname_train = os.path.join(\"visualize_train\")\n",
    "# cmd = (\n",
    "#     \"scripts/combat_visualize_data.py\"\n",
    "#     + \" \"\n",
    "#     + temp_train_file_original\n",
    "#     + \" \"\n",
    "#     + temp_train_file\n",
    "#     + \" --out_dir \"\n",
    "#     + directory\n",
    "#     + \" --outname \"\n",
    "#     + outname_train\n",
    "#     + \" -f\"\n",
    "#     + \" --bundles all\"\n",
    "# )\n",
    "# subprocess.call(cmd, shell=True)\n",
    "\n",
    "# # Display gammas and deltas along with their mean and standard deviation\n",
    "# print(\"Gammas:\", gammas)\n",
    "# print(\"Deltas:\", deltas)\n",
    "# gammas = list(gammas.values())\n",
    "# deltas = list(deltas.values())\n",
    "# print(\"\\nGamma Statistics:\")\n",
    "# print(f\"Mean: {np.mean(gammas)}, Std: {np.std(gammas)}\")\n",
    "\n",
    "# print(\"\\nDelta Statistics:\")\n",
    "# print(f\"Mean: {np.mean(deltas)}, Std: {np.std(deltas)}\")\n",
    "# print(\"Ruffles:\", ruffles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST Powerpoint generation\n",
    "# d  = os.path.join(MAINFOLDER, robust_method, \"adni_100_Philips_3T\")\n",
    "# create_presentation(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST the sample_patients function with compilation data data\n",
    "# sampled_df = sample_patients(COMPILATION, num_patients=100, disease_ratio=0.5)\n",
    "# print(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_metrics(\"ROBUST/IQR/50_30/0/\", \"50_patients_30_percent_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dists_compilation and metrics_compilation CSV files\n",
    "# dists_compilation_path = os.path.join(directory, \"dists_compilation.csv\")\n",
    "# metrics_compilation_path = os.path.join(directory, \"metrics_compilation.csv\")\n",
    "\n",
    "# dists_compilation = pd.read_csv(dists_compilation_path)\n",
    "# metrics_compilation = pd.read_csv(metrics_compilation_path)\n",
    "\n",
    "# # Change the site column\n",
    "# dists_compilation['site'] = dists_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "# metrics_compilation['site'] = metrics_compilation['site'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# # Display the means by site\n",
    "# dists_means_by_site = dists_compilation.groupby(['site','comparaison']).mean()\n",
    "# metrics_means_by_site = metrics_compilation.groupby('site').mean()\n",
    "\n",
    "# print(dists_means_by_site)\n",
    "# print(metrics_means_by_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIX METRICS COMPILATION\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# df = pd.read_csv(os.path.join(directory, \"metrics_compilation.csv\"))\n",
    "\n",
    "# # Group by the site\n",
    "# grouped = df.groupby('site')\n",
    "\n",
    "# # Process each site\n",
    "# cleaned_dfs = []\n",
    "# for site, group in grouped:\n",
    "#     # Reset index for easier manipulation\n",
    "#     group = group.reset_index(drop=True)\n",
    "    \n",
    "#     # # The first row is the \"bundle row\" (new column names)\n",
    "#     # new_columns = group.iloc[0].values  # Extract column names from the first row\n",
    "#     # new_columns[-1] = 'site'\n",
    "#     # group = group.iloc[1:]  # Remove the first row\n",
    "    \n",
    "#     # # Assign new column names\n",
    "#     # group.columns = new_columns\n",
    "    \n",
    "#     # # Sort the columns alphabetically (excluding 'site')\n",
    "#     # sorted = group.sort_index(axis=1)\n",
    "#     # Add a new column 'nomm' with the value indicating the metric for each row\n",
    "#     metrics = ['tp', 'fp', 'tn', 'fn', 'precision', 'recall', 'taux_faux_positifs', 'f1_score']\n",
    "#     group['metric'] = metrics\n",
    "    \n",
    "#     # # Append the cleaned DataFrame for this site\n",
    "#     cleaned_dfs.append(group)\n",
    "\n",
    "# # Concatenate all cleaned DataFrames\n",
    "# final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "# # Save or display the result\n",
    "# final_df.to_csv(os.path.join(directory, \"metrics_compilation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL SITES\n",
    "# directory = os.path.join(MAINFOLDER, robust_method)\n",
    "# raw_directory = os.path.join(RAWFOLDER, site_group)\n",
    "# for filename in sorted(os.listdir(raw_directory)):\n",
    "#     f = os.path.join(raw_directory, filename)\n",
    "#     # checking if it is a file\n",
    "#     if os.path.isfile(f):\n",
    "#         analyse_site(f, robust_method, directory)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
