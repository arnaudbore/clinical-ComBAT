{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diagnostic MLP – 430 Features\n",
        "\n",
        "Version 0.2  \n",
        "Auteur : Yoan  \n",
        "Date : 2025‑06‑30\n",
        "\n",
        "Objectif : prédire automatiquement si un patient est malade (1) ou sain (0) à l’aide de 430 caractéristiques numériques extraites de données d’IRM.\n",
        "\n",
        "Le notebook suit un pipeline complet : ingestion, feature engineering, split, hyperparameter tuning, entraînement final, évaluation et explicabilité.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration générale ---\n",
        "from pathlib import Path\n",
        "from robust_evaluation_tools.robust_MLP import PatientMLP, MODEL_DIR\n",
        "DATA_DIR  = Path(\"DONNES_F/COMPILATIONS_AUG_3/\")      # <-- adapte si besoin\n",
        "disease = \"ALL\"\n",
        "RUN_NAME  = f\"mlp2_{disease}\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SEED = 41\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSTALLATION (décommente si nécessaire)\n",
        "# %pip install -q pandas numpy scikit-learn torch optuna shap tensorboard joblib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, roc_auc_score, f1_score,\n",
        "                             confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay,\n",
        "                             PrecisionRecallDisplay)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import joblib, random, os, json, optuna\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----- Helpers -----\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(SEED)\n",
        "\n",
        "device = \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def show_class_balance(y):\n",
        "    vals, counts = np.unique(y, return_counts=True)\n",
        "    for v, c in zip(vals, counts):\n",
        "        print(f\"Classe {int(v)} : {c}\")\n",
        "\n",
        "def plot_curves(train, val, ylabel=\"Loss\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    epochs = range(1, len(train)+1)\n",
        "    plt.plot(epochs, train, label=\"train\")\n",
        "    plt.plot(epochs, val,   label=\"val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(f\"Courbe {ylabel}\")\n",
        "    plt.legend(); plt.grid(True); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Si disease == \"ALL\", on fusionne toutes les maladies sans doublons de SID\n",
        "if disease == \"ALL\":\n",
        "    # sids_vus = set()\n",
        "    # df_total = pd.DataFrame()\n",
        "    # for maladie in [\"AD\", \"ADHD\", \"BIP\", \"MCI\", \"SCHZ\", \"TBI\"]:\n",
        "    #     df_raw = pd.read_csv(DATA_DIR / f\"{maladie}_combination_all_metrics_CamCAN.csv.gz\")\n",
        "        \n",
        "    #     # On enlève les SIDs déjà vus\n",
        "    #     df_filtré = df_raw[~df_raw[\"sid\"].isin(sids_vus)]\n",
        "        \n",
        "    #     # On ajoute les nouveaux SIDs à notre set\n",
        "    #     sids_vus.update(df_filtré[\"sid\"].unique())\n",
        "        \n",
        "    #     # On concatène le DataFrame filtré\n",
        "    #     df_total = pd.concat([df_total, df_filtré], ignore_index=True)\n",
        "    # df_raw = df_total\n",
        "    df_raw = pd.read_csv(\"DONNES_MLP/train_data_all_aug5.csv\")\n",
        "    df_raw[~((df_raw['disease'] == 'HC') & (df_raw['old_site'] != 'CamCAN'))]\n",
        "else:\n",
        "    df_raw = pd.read_csv(DATA_DIR / f\"{disease}_combination_all_metrics_CamCAN.csv.gz\")\n",
        "print(\"Raw shape:\", df_raw.shape)\n",
        "display(df_raw.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nettoyage minimal\n",
        "df_raw = df_raw[~df_raw['bundle'].isin(['left_ventricle', 'right_ventricle'])].copy()\n",
        "print(\"Sans ventricules :\", df_raw.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 3. Feature engineering -----\n",
        "def compute_zscore(df, value_col=\"mean_no_cov\"):\n",
        "    stats = (df.groupby(\"metric_bundle\")[value_col]\n",
        "               .agg(['mean', 'std'])\n",
        "               .rename(columns={'mean': 'global_mean', 'std': 'global_std'}))\n",
        "    stats['global_std'] = stats['global_std'].replace(0, 1e-6)\n",
        "    df = df.merge(stats, on=\"metric_bundle\", how=\"left\")\n",
        "    df[\"zscore\"] = (df[value_col] - df[\"global_mean\"]) / df[\"global_std\"]\n",
        "    return df.drop(columns=[\"global_mean\", \"global_std\"])\n",
        "\n",
        "def build_feature_matrix(df, value_col=\"zscore\", bundle_col=\"metric_bundle\", healthy_tag=\"HC\"):\n",
        "    features = df.pivot(index=\"sid\", columns=bundle_col, values=value_col)\n",
        "    label = (df.groupby(\"sid\")[\"disease\"].first().ne(healthy_tag).astype(int))\n",
        "    mat = features.assign(label=label).reset_index(drop=False)\n",
        "    return mat\n",
        "\n",
        "df_clean = compute_zscore(df_raw, value_col=\"mean_no_cov\")\n",
        "dupes = (df_clean\n",
        "         .groupby([\"sid\", \"metric_bundle\"])\n",
        "         .size()\n",
        "         .loc[lambda s: s > 1]\n",
        "         .sort_values(ascending=False))\n",
        "print(f\"Nombre de paires sid / metric_bundle en double : {dupes.shape[0]}\")\n",
        "df_mat   = build_feature_matrix(df_clean, value_col=\"zscore\")\n",
        "df_mat = df_mat.drop(columns=[\"sid\"])\n",
        "print(\"Matrix shape:\", df_mat.shape)\n",
        "display(df_mat.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 4. Split & normalisation -----\n",
        "X = df_mat.drop(columns=\"label\").values.astype(np.float32)\n",
        "y = df_mat[\"label\"].values.astype(np.float32)\n",
        "show_class_balance(y)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.5, stratify=y, random_state=SEED)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
        "\n",
        "# scaler = StandardScaler().fit(X_train)\n",
        "# X_train = scaler.transform(X_train)\n",
        "# X_val   = scaler.transform(X_val)\n",
        "# X_test  = scaler.transform(X_test)\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 5. DataLoader -----\n",
        "class PatientDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "BATCH = 64\n",
        "train_dl = DataLoader(PatientDataset(X_train, y_train), batch_size=BATCH, shuffle=True)\n",
        "val_dl   = DataLoader(PatientDataset(X_val,   y_val),   batch_size=BATCH)\n",
        "test_dl  = DataLoader(PatientDataset(X_test,  y_test),  batch_size=BATCH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 6A. Baseline LogisticRegression -----\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "baseline = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "baseline.fit(X_train, y_train)\n",
        "prob_val = baseline.predict_proba(X_val)[:,1]\n",
        "auc_base = roc_auc_score(y_val, prob_val)\n",
        "print(f\"AUC validation LogisticRegression: {auc_base:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 7. Training helpers -----\n",
        "def train_epoch(model, loader, crit, opt):\n",
        "    model.train()\n",
        "    running = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = crit(model(xb), yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running += loss.item() * xb.size(0)\n",
        "    return running / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, crit):\n",
        "    model.eval()\n",
        "    losses, probs, labels = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits, yb.to(device)).item()\n",
        "        losses.append(loss * xb.size(0))\n",
        "        probs.append(torch.sigmoid(logits).cpu())\n",
        "        labels.append(yb)\n",
        "    probs  = torch.cat(probs).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "    auc = roc_auc_score(labels, probs)\n",
        "    f1  = f1_score(labels, (probs>0.5).astype(int))\n",
        "    return np.sum(losses) / len(loader.dataset), auc, f1\n",
        "\n",
        "def fit(model, train_dl, val_dl, epochs=100, lr=1e-3, wd=1e-4, patience=10, run_name=\"run\"):\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    opt  = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched= torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
        "    writer = SummaryWriter(f\"{MODEL_DIR}/runs/{run_name}\")\n",
        "    best_auc, best_state, counter = 0, None, 0\n",
        "    tr_losses, val_losses = [], []\n",
        "    for ep in tqdm(range(1, epochs+1)):\n",
        "        tr_loss = train_epoch(model, train_dl, crit, opt)\n",
        "        val_loss, val_auc, _ = eval_epoch(model, val_dl, crit)\n",
        "        tr_losses.append(tr_loss); val_losses.append(val_loss)\n",
        "        writer.add_scalar(\"Loss/train\", tr_loss, ep)\n",
        "        writer.add_scalar(\"Loss/val\",   val_loss, ep)\n",
        "        writer.add_scalar(\"AUC/val\",    val_auc,  ep)\n",
        "        sched.step(val_loss)\n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            best_state = model.state_dict()\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    model.load_state_dict(best_state)\n",
        "    return best_state, tr_losses, val_losses, best_auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 8. Hyperparameter tuning (Optuna) -----\n",
        "def objective(trial):\n",
        "    hidden_dim1 = trial.suggest_int(\"h1\", 128, 512, step=64)\n",
        "    hidden_dim2 = trial.suggest_int(\"h2\", 64, 256, step=32)\n",
        "    hidden_dim3 = trial.suggest_int(\"h3\", 32, 128, step=16)\n",
        "    drop        = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
        "    wd          = trial.suggest_float(\"wd\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    model = PatientMLP(hidden_dims=(hidden_dim1, hidden_dim2, hidden_dim3), drop=drop).to(device)\n",
        "    state, _, _, best_auc = fit(model, train_dl, val_dl,\n",
        "                                epochs=15, lr=lr, wd=wd,\n",
        "                                patience=5, run_name=\"tune\")\n",
        "    return best_auc\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(\"Best AUC:\", study.best_value)\n",
        "print(\"Best params:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 9. Entraînement final avec les meilleurs hyperparamètres -----\n",
        "best = study.best_params\n",
        "model_final = PatientMLP(\n",
        "    hidden_dims=(best[\"h1\"], best[\"h2\"], best[\"h3\"]),\n",
        "    drop=best[\"dropout\"]).to(device)\n",
        "state, train_losses, val_losses, best_auc = fit(\n",
        "    model_final, train_dl, val_dl,\n",
        "    epochs=100, lr=best[\"lr\"], wd=best[\"wd\"],\n",
        "    patience=10, run_name=RUN_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Courbes d’apprentissage\n",
        "plot_curves(train_losses, val_losses, ylabel=\"BCE Loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 11. Évaluation finale sur test -----\n",
        "_, test_auc, test_f1 = eval_epoch(model_final, test_dl, nn.BCEWithLogitsLoss())\n",
        "print(f\"AUC test: {test_auc:.3f} | F1 test: {test_f1:.3f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "model_final.eval()\n",
        "preds, labels = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_dl:\n",
        "        preds.append(torch.sigmoid(model_final(xb.to(device))).cpu())\n",
        "        labels.append(yb)\n",
        "preds = torch.cat(preds).numpy()\n",
        "labels= torch.cat(labels).numpy()\n",
        "ConfusionMatrixDisplay.from_predictions(labels, preds>0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 12. Sauvegarde -----\n",
        "torch.save(state, MODEL_DIR / f\"{RUN_NAME}_weights.pt\")\n",
        "with open(MODEL_DIR / f\"{RUN_NAME}_params.json\", \"w\") as fp:\n",
        "    json.dump(study.best_params, fp, indent=2)\n",
        "print(\"Artifacts saved in\", MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 13. Exemple d’inférence -----\n",
        "sample = np.random.rand(430).reshape(1, -1)\n",
        "with torch.no_grad():\n",
        "    prob = torch.sigmoid(model_final(torch.tensor(sample, dtype=torch.float32).to(device))).item()\n",
        "print(f\"Probabilité malade: {prob:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- 14. Explainability (facultatif) -----\n",
        "# import shap\n",
        "# explainer = shap.DeepExplainer(model_final, torch.tensor(X_train[:100]).to(device))\n",
        "# shap_values = explainer.shap_values(torch.tensor(sample_std).to(device))\n",
        "# shap.summary_plot(shap_values, features=sample_std)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
